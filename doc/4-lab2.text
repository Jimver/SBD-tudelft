# Lab 2 {.unnumbered}

In the first lab you built an application for a small dataset to analyze the
most common topics in the news according to the GDelt dataset. In this lab we
will scale this application using Amazon Web Services to process the entire
dataset (several terabytes). You are free to pick either your RDD, or
Dataframe/Dataset implementation.

## Before you start

**We assume everybody has access to AWS credits via both the GitHub developer
pack and the AWS classroom in their lab group! If this is not the case, please
ask for help, or send us an email.**

**You pay for cluster per commissioned minute. After you are done working with
a cluster, please terminate the cluster, to avoid unnecessary costs.**

Make sure you have read the introduction on Amazon Web services in the guide
chapter and that you have a working solution to lab 1 before starting the
assignment.

## Assignment

For this lab, we would like you to process the entire dataset, meaning all
segments, with 20 `c4.8xlarge` core nodes, in under half an hour, using your
solution from lab 1. This should cost you less than 12 dollars and is the
minimum requirement.

Note that this means that you should evaluate whether you application scales
well enough to achieve this before attempting to run it on the entire dataset.
Your answers to the questions in lab 1 should help you to determine this, but
to reiterate: consider e.g.Â how long it will take to run

1. 1000 segments compared to 10000 segments?
2. on 4 virtual cores compared to 8, and what about 32?

If your application is not efficient enough right away, you should analyze its
bottlenecks and modify it accordingly, or try to gain some extra performance by
modifying the way Spark is configured.
You can try a couple of runs on the entire dataset when you have a good
understanding of what might happen on the entire dataset.

For extra points, we challenge you to come up with an even better solution
according to the metric you defined in lab 1. You are free to change anything,
but some suggestions are:

-   Find additional bottlenecks using Apache Ganglia (need more network I/O, or
    more CPU?, more memory?)
-   Tuning the kind and number of machines you use on AWS, based on these
    bottlenecks
-   Modifying the application to increase performance
-   Tuning Yarn/Spark configuration flags to best match the problem

There is a [guide to Spark performance] tuning on the Spark website.


## Deliverables

The deliverable for this lab is a blog post outlining your choices in
terms of configuration and your results in achieving the assignment. Be concise
in your blog, you can skip introductions and go straight to juicy bits. These
blog posts should be written in [Github flavored markdown], as we will
publish all reports so you can learn from your classmates. We encourage you to
have a look at them after we publish these!

In the blog post, there should be a justification for why you chose the cluster
configuration you did. If you have measurements for multiple cluster
configurations, please include them. Also detail all the improvements you
found, and why they improved effectiveness.

  [Github flavored markdown]: https://help.github.com/en/articles/basic-writing-and-formatting-syntax
